<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  <link rel="shortcut icon" href="../../img/favicon.ico">
  <title>Logistic_regression的Python代码实现 - Darren_blog</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="../../css/highlight.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Logistic_regression\u7684Python\u4ee3\u7801\u5b9e\u73b0";
    var mkdocs_page_input_path = "machine_learning/logistic regression.md";
    var mkdocs_page_url = "/machine_learning/logistic regression/";
  </script>
  
  <script src="../../js/jquery-2.1.1.min.js"></script>
  <script src="../../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../../js/highlight.pack.js"></script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="../.." class="icon icon-home"> Darren_blog</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="../..">Darren_blog</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../../data_analysis/lagou_job_analysis_forshow/">拉勾招聘职位数据分析</a>
	    </li>
          
            <li class="toctree-l1 current">
		
    <a class="current" href="./">Logistic_regression的Python代码实现</a>
    <ul class="subnav">
            
    <li class="toctree-l2"><a href="#mathbbpython">\mathbb{逻辑回归与python 代码实现}</a></li>
    
        <ul>
        
            <li><a class="toctree-l3" href="#_1">线性回归分析</a></li>
        
            <li><a class="toctree-l3" href="#logistic-regression">逻辑回归（Logistic Regression）</a></li>
        
            <li><a class="toctree-l3" href="#pythonsklearn">python代码示例(调用sklearn)</a></li>
        
            <li><a class="toctree-l3" href="#_4">参考文献</a></li>
        
        </ul>
    

    </ul>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../..">Darren_blog</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../..">Docs</a> &raquo;</li>
    
      
    
    <li>Logistic_regression的Python代码实现</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="mathbbpython">
<script type="math/tex; mode=display">\mathbb{逻辑回归与python 代码实现}</script>
</h1>
<p>逻辑回归(Logistic Regression, LR)又称为逻辑回归分析，是分类和预测算法中的一种。通过历史数据的表现对未来结果发生的概率进行预测。</p>
<p>例如，我们可以将商品的购买概率设置为因变量，将用户的特征属性，例如性别，年龄，注册时间等设置为自变量。根据特征属性预测购买的概率。逻辑回归与线性回归分析（Linear Regression）有很多相似之处，下面先来看下线性回归分析</p>
<h2 id="_1">线性回归分析</h2>
<p style="display:inline-block; font-weight:bold;color:purple;">首先，温故一下直线方程</p>

<p>
<script type="math/tex; mode=display"> y = ax + b </script>
</p>
<p>自变量x乘以斜率a再加上截距b就得到了因变量y</p>

<pre><code class="python">import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
# from matplotlib.font_manager import FontProperties
x = np.arange(-10, 10)
a, b = 2, 5
fig, ax = plt.subplots(figsize=(8, 8))
# 隐藏上边和右边
ax.spines[&quot;top&quot;].set_color(&quot;none&quot;) 
ax.spines[&quot;right&quot;].set_color(&quot;none&quot;) 
# 移动另外两个轴
ax.xaxis.set_ticks_position(&quot;bottom&quot;)
ax.spines['bottom'].set_position(('data', 0))
ax.yaxis.set_ticks_position('left')
ax.spines['left'].set_position(('data', 0))
ax.plot(x, a*x+b, linewidth=2, label=&quot;y=2x+5&quot;)
ax.legend()
ax.grid(True, linestyle=&quot;:&quot;, linewidth=1.5, alpha=0.8)
</code></pre>

<p><img alt="png" src="../output_4_0.png" /></p>
<p><b>一元一次方程</b> <script type="math/tex; mode=display">\quad y \ = \ ax \ + \ b \quad 例如：\ y = 2x + 5</script>
</p>
<p>可以写成：  <script type="math/tex; mode=display">\quad y \ = \ w_0 \times x_0 \ + \ w_1 \times x_1 \quad 其中w_0 =5, \ x_0=1, \ w_1=2, \ x_1=x</script>
</p>
<p><b>二元一次方程</b> <script type="math/tex; mode=display">\quad y \ = \ ax \ + \ bx \ + \ c </script>
</p>
<p>可以写成：  <script type="math/tex; mode=display">\quad y \ = \ w_0 \times x_0 \ + \ w_1 \times x_1 \ + \ w_2 \times x_2 \quad 其中x_0=1, \ w_0=c</script>
</p>
<p><b>n元一次方程表达式及矩阵表示：</b></p>
<p>
<script type="math/tex; mode=display">y \ = \ w_0\times x_0 + w_1 \times x_1 + w_2 \times x_2 + \cdot\cdot\cdot \ w_n \times x_n \ </script>
</p>
<p>
<script type="math/tex; mode=display">=\underbrace{\begin{bmatrix} w_0 & w_1 & w_2 & \cdots\ &w_n \end{bmatrix}}_{权重系数向量\bf w} {\ \bullet}  \underbrace{\begin{bmatrix} x_{0} \\ x_{1} \\ x_{2} \\ \vdots \\ x_n \end{bmatrix}}_{样本特征矩阵\bf x}</script>
</p>
<p>
<script type="math/tex; mode=display">=\ {\bf w^T x} \quad {其中x_0=1}</script>
</p>
<p><b>回归分析用来描述自变量x和因变量Y之间的关系，或者说自变量X对因变量Y的影响程度，并对因变量Y进行预测。</b> 
其中因变量(y)是我们希望获得的结果，自变量(x)是影响结果的潜在因素，自变量可以有一个，也可以有多个。一个自变量的叫做一元回归分析，超过一个自变量的叫做多元回归分析。</p>
<p>回归分析其实就是对已知公式的未知参数进行估计，在给定训练样本点和已知的公式后，去求解一个或多个未知参数，直到找到那个最符合样本点分布的参数（或参数组合）。注意，回归的前提是公式已知，否则回归无法进行。根据公式的不同，回归分为线性回归和非线性回归。线性回归中公式都是“一次”的（一元一次方程，二元一次方程...），而非线性则可以有各种形式（N元N次方程，log方程...）。</p>
<p>下面是一组广告费用和曝光次数的数据，费用和曝光次数一一对应。其中曝光次数是我们希望知道的结果，费用是影响曝光次数的因素，我们将费用设置为自变量X，将曝光次数设置为因变量Y，通过一元线性回归方程和判定系数可以发现费用(X)对曝光次数(Y)的影响。
<br>
<img src="./image/liner_reg_adv.png"></p>
<p>以下为一元回归线性方式表达形式，<script type="math/tex; mode=display">\hat{y} \ = \ w_0 \ + \ w_1 x_1</script>
</p>
<p>其中$\bf{\hat{y}}$是预测的因变量，$\bf{x_1}$是自变量$\bf{X}$的一个具体值，我们只需求出截距$w_0$和斜率$w_1$就可以获得费用和曝光次数之间的关系，就可以对曝光次数进行预测。通常，可以用用最小二乘法来计算截距b0和斜率b1。最小二乘法通过最小化误差的平方和寻找数据的最佳函数匹配。</p>
<p><img src="./image/linear_regression.png"></p>
<p>关于几个小例子可以点击<a href="http://www.cnblogs.com/nxld/p/6124235.html">这里</a>和<a href="http://blog.csdn.net/wenyusuran/article/details/25824011">这里</a></p>
<h2 id="logistic-regression">逻辑回归（Logistic Regression）</h2>
<p>Logistic Regression 是线性回归的一种，是工业界比较常用的有监督式的分类机器学习算法，用于估计某种事物的可能性，比如，用于广告预测（ctr预估），根据不同广告的点击的历史数据，来预测某广告的点击率的可能性，然后，把最可能被用户点击的广告摆在用户能看到的地方，当用户点击了该广告，网站就有钱收了。</p>
<p>现实生活中，我们不仅需要回归产生一个可以有很多变化的因变量，有时还需要产生类似概率值的0~1之间的数值（比如某一双鞋子今天能否卖出去？或者某一个广告能否被用户点击? 我们希望得到这个数值来辅助决策鞋子上不上架，以及广告展不展示）。这个数值必须是0~1之间，但显然线性回归不满足这个区间要求。于是引入了Logistic方程，来做归一化，即产生了逻辑回归。</p>
<p>在介绍逻辑回归之前，我们先看看一个小例子：</p>
<p>鸢尾花数据集包括三种鸢尾花，山鸢尾花(Iris Setosa)、变色鸢尾花(Iris　Versicolor)、维吉尼亚鸢尾花(Iris Virginica)</p>
<p><img src="./image/Iris Setosa.jpeg" alt="山鸢尾花(Iris Setosa)" style="width:250px;height:250px;display:inline-block;margin-top:20px;">
<img src="./image/Iris Versicolor.jpeg" alt="变色鸢尾花(Iris　Versicolor)" style="width:250px;height:250px;display:inline-block;">
<img src="./image/Iris Virginica.jpeg" alt="维吉尼亚鸢尾花(Iris Virginica)" style="width:250px;height:250px;display:inline-block;"></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;山鸢尾花(Iris Setosa) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;变色鸢尾花(Iris　Versicolor) &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;维吉尼亚鸢尾花(Iris Virginica)</p>

<p><b style="color:purple">搞清几个概念：</b></p>
<p><b>
样本</p>
<p>特征</p>
<p>分类标签</p>
<p>训练集(train)</p>
<p>测试集(test)
</b></p>
<p>莺(dai)尾花数据集:
<script type="math/tex; mode=display">150行\ \times \ 5列</script>
</p>
<p>特征：4个，分别是花瓣长度和宽度，花萼长度和宽度，单位cm</p>
<p>分类标签：最后一列，三种取值，分别代表三种类型的莺尾花</p>
<p><img src="./image/iris_petal_sepal.png"></p>
<pre><code class="python">import pandas as pd
df = pd.read_excel(&quot;./Iris.xls&quot;, sheetname=&quot;Iris&quot;)
print(type(df))
print(df.shape)
df.head(4)
#df.tail(4)
#df.iloc[49:54, :]
</code></pre>

<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
(150, 5)
</code></pre>
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sepal length</th>
      <th>sepal width</th>
      <th>petal length</th>
      <th>petal width</th>
      <th>iris</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>Iris-setosa</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4.9</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>Iris-setosa</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.7</td>
      <td>3.2</td>
      <td>1.3</td>
      <td>0.2</td>
      <td>Iris-setosa</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.6</td>
      <td>3.1</td>
      <td>1.5</td>
      <td>0.2</td>
      <td>Iris-setosa</td>
    </tr>
  </tbody>
</table>
</div>

<h3 id="sigmoid">sigmoid函数</h3>
<p>sigmoid函数，数学表达式：
<script type="math/tex; mode=display">s\left(h\right) \ = \ \frac{1}{1 \ + \ e^{-h}}</script>
注意，h在这不仅仅代表我们通常意义的自变量x，还可代表多个自变量的组合。
<img src="./image/Logistic_curve.png" style="width:500px;height:300px;"></p>
<p>Logistic regression可以用来做回归，也可以用来分类（主要用于二分类）。从上图可以看到sigmoid函数是一个s形的曲线，$s\left(h\right)$的取值在(0, 1)之间，当h=0时，$\ s\left(h\right)=0.5\ $,在h远离0的地方函数的值会很快接近0或1。这个性质使我们能够以概率的方式来解释。</p>
<h3 id="sigmoid_1">sigmoid函数推导</h3>
<p>其实，相比线性回归方程，逻辑回归是在线性回归的基础上增加了一个逻辑函数，即将<script type="math/tex; mode=display">\ y \ = \ ax \ + \ b \ </script>作为自变量带入了sigmoid函数里（此时<script type="math/tex; mode=display">h=y= ax \ + \ b </script>）。
从一个例子说起,我们通过用户的属性和特征来判断用户最终是否会进行购买一种商品。其中购买的概率是因变量y，用户的属性和特征是自变量X。y值越大说明用户购买的可能性越大。这里我们使用InOdds(E)表示这个购买事件，该事件发生的可能性（odds）来表示购买（P(E) &lt;=&gt; P）与未购买（P(E')）的可能性比值，公式如下：
<script type="math/tex; mode=display">InOdds\left(E\right)=W_0 + W_1 \times X_1 + W_2 \times X_2 + \cdots + W_n \times X_n +  \epsilon</script>
<script type="math/tex; mode=display">Odds\left(E\right) = \frac{P \left(E \right)}{P \left(E' \right)} = \frac{P \left(E \right)}{1 - P \left(E \right)} = \frac{P}{1 - P}</script>
</p>
<p>Odds是一个从0到无穷的数字，Odds的值越大，表明事件发生的可能性越大。下面我们要将Odds转化为0-1之间的概率函数。首先对Odds取自然对数，得到logit方程，logit是一个范围在负无穷到正无穷的值。</p>
<p>
<script type="math/tex; mode=display">logit(p) = lnOdds(p) = ln \frac{p}{1 - p} = lnp - ln(1-p)</script>
</p>
<p><img src="./image/Logit.svg.png"></p>
<p>基于上面的logit方程，获得以下公式：</p>
<p>
<script type="math/tex; mode=display">logit(\pi) = lnOdds(\pi) = ln\frac{P(E)}{P(1-E)} = W_0 + W_1 \times X_1 + W_2 \times X_2 + \cdots + W_n \times X_n +  \epsilon</script>
</p>
<p>其中使用了$\pi$替换了公式中的P(E)，即$\pi = P(E)$。根据指数函数和对数函数规则得到以下公式：</p>
<p>
<script type="math/tex; mode=display">\frac{P(E)}{1 - P(E)} = Odds(E) = e^{W_0 + W_1 \times X_1 + W_2 \times X_2 + \cdots + W_n \times X_n +  \epsilon}</script>
</p>
<p>移项，化简得到逻辑回归方程：</p>
<p>
<script type="math/tex; mode=display"> p = P(E) = \frac{e^{w_0 + w_1 \times x_1 + w_2 \times x_2 + \cdots + w_n \times x_n}}{1 + e^{w_0 + w_1 \times x_1 + w_2 \times x_2 + \cdots + w_n \times x_n}} = \frac{1}{1 + e^{-(w_0 + w_1 \times x_1 + w_2 \times x_2 + \cdots + w_n \times x_n)}} </script>
</p>
<p>e(1-p) = p
e - ep = p 
p(1 + e) = e
p = e / 1 + e</p>
<h3 id="_2">逻辑回归模型解读</h3>
<p><b>逻辑回归函数：</b> 
<script type="math/tex; mode=display">\quad s\left(h\right) \ = \ \frac{1}{1 \ + \ e^{-h}}\quad</script> 　<div id="func" name="func">其中,$\quad h = w_0 + w_1 \times x_1 + w_2 \times x_2 + \cdots + w_n \times x_n$</div></p>
<p>假设有n个样本{$\bf X$, y}，y是分类标记，取值是0或1，表示负类还是正类，$\bf X$是m维的样本特征向量，那么这个样本$\bf X$属于正类，也就是y=1的“概率”可以通过下面的逻辑函数来表示：</p>
<p>
<script type="math/tex; mode=display">p(y=1 \mid x; w) = s(w^T x) = \frac{1}{1 + e^{- w^T x}}</script>
这里的$\bf w$是模型参数，也称回归系数，$\ s$是sigmoid函数。</p>
<p><b>决策函数是：</b>
<script type="math/tex; mode=display">\phi\left(x\right)=\left\{
\begin{aligned}
1 &\ \quad if\ p(y=1 \mid x)\geq 0.5 \\
0 &\ \quad otherwise.
\end{aligned}
\right.
</script>
通常，我们选择0.5作为阈值，当有特定的需求时可以选择不同阈值，如果对正例的判别准确性要求高，可以选择阈值大一些，对正例的召回要求高，则可以选择阈值小一些。</p>
<p><b style="margin-top:40px; display:block; font-size:22px;">模型参数求解</b>
<p style="color:purple; font-weight=bold">最大似然估计<p>
<hr style="border:1px dotted purple;">
我们要用逻辑回归函数去做分类，必须要求出模型系数<script type="math/tex; mode=display">\bf w</script>，那么如何求解<script type="math/tex; mode=display">\bf w</script>呢？
答案是最大似然估计（maximum likelihood）+ 梯度下降（gradient descent）。</p>
<p>最大似然估计的本质是，选择最佳的参数<script type="math/tex; mode=display">\bf w</script>,来最大化样本数据的可能性。假设给定样本<script type="math/tex; mode=display">X_1, X_2 \cdots X_n</script>, 那么关于参数<script type="math/tex; mode=display">\bf w</script>的可能性函数(可能性函数就是样本数据作为参数w的函数的概率)：
<script type="math/tex; mode=display">lik(w) = f(X_1, X_2, X_3, \cdots X_n \mid w)</script>
如果<script type="math/tex; mode=display">X_1, X_2, X_3, \cdots X_n</script>之间是相互独立的，可能性函数可以简写为如下：
<script type="math/tex; mode=display">lik(w) = \prod_{i=1}^n f(X_i \mid w) \quad likelihood \ function</script>
</p>
<p>一般情况，我们要使用log可能性函数，原因：</p>
<p>1.对上面的likelihood function两边同时取对数，就得到了log likelihood function，这样乘积转换为求和，从而使得函数的求导更容易；</p>
<p>2.如果我们有很多的样本数据，若直接用likelihood function，该函数是连乘的，而且通常这些项都较小，故可能性函数就会变得很小。所以，应该采用log可能性函数，可以防止当样本可能性很小时，可能出现的数值下溢;</p>
<p>3.log函数是单调的，最大化可能性函数的值也就是最大化log可能性函数的值。log可能性函数公式如下：
<script type="math/tex; mode=display">l(w) = log(\ lik(w)\ ) = \sum_{i=1}^n log(\ f(X_i \mid w)\ )</script>
</p>
<p>用可能性函数来定义上面的模型系数w，要利用二项分布的概率密度函数，公式如下：
<script type="math/tex; mode=display">L(w) = \prod_{i=1}^n p(y^{(i)} \mid x^{(i)};w) = 
\prod_{i=1}^n (s(h^{(i)})^{y^{(i)}} (1 - s(h^{(i)}))^{1 - y^{(i)}}</script>
其中，$y^{(i)}$表示第i个样本对应的分类标记值，$x^{(i)}$表示第i个样本的特征，$h^{(i)}$为第i个样本对应的假设函数的值，$h = h(w) = w_0 + w_1 \times x_1 + w_2 \times x_2 + \cdots + w_n \times x_n\ $ 上面式子两边同时求log，得到最终的log可能性函数：
<script type="math/tex; mode=display">l(w) = log(L(w)) = \sum_{i=1}^n y^{(i)} log(s(h^{(i)})) + (1 - y^{(i)}) log(1 - s(h^{(i)}))</script>
<p style="color:purple; font-weight=bold">逻辑回归的cost函数<p>
<hr style="border:1px dotted purple;">
最大似然估计就是要求得使l(w)取最大值时的w，这里可以使用梯度上升法求解，若要用梯度下降法，就要乘以一个负数，在这，我们可以先直接乘以-1得到如下公式：
<script type="math/tex; mode=display">J(w) = -l(w) = -log(L(w)) = \sum_{i=1}^n -y^{(i)} log(s(h^{(i)})) - (1 - y^{(i)}) log(1 - s(h^{(i)}))</script>
现在，我们就分析一个样本，那么可以把上面的cost函数写成如下形式：</p>
<p>
<script type="math/tex; mode=display">J(w) = -ylog(s(h)) - (1 - y)log(1 - s(h))</script>
</p>
<p>當y=0時，上式的前面一項$-ylog(s(h)) = 0$，而當y=1時，後面一項$(1 - y)log(1 - s(h)) = 0$，故上式可寫成如下分段函数：</p>
<p>
<script type="math/tex; mode=display">\phi\left(x\right)=\left\{
\begin{aligned}
-log(s(h)) &\ \quad if\ y = 1 \\
-log(1 - s(h)) &\ \quad if\ y = 0
\end{aligned}
\right.
</script>
<img src="./image/cost_function.png" style="width:600px;height:400px;diplay:block; padding-bottom:20px;">
the previous plot illustrates the cost for the classification of a single-sample instance for different values of s(h).
好了，现在的目标是最小化cost函数，找到最佳的参数$\bf w$，方法是梯度下降法。
<br>
<p style="color:purple; font-weight=bold">梯度下降法求模型参数$\bf w$<p>
<hr style="border:1px dotted purple;"></p>
<p>对l(w)函数乘以$-\frac{1}{n}$，n是样本数量, 所以，cost函数为：
<script type="math/tex; mode=display">J(w) = -\frac{1}{n}l(w)</script>
正因为乘了一个负的系数$-\frac{1}{n}$，所以J(w)取最小值时的w为要求的最佳参数。根据梯度下降法可得到$\bf w$的更新规则如下：
<script type="math/tex; mode=display">\bf w_j: = w_j \ + \ \Delta {\bf w_j}</script>
</p>
<p>
<script type="math/tex; mode=display">\Delta \bf w_j = -\eta \nabla J(w)= -\eta \frac{\partial J(w)}{\partial w_j} = \eta\left(y^\left(i\right) - h_w\left(x^{(i)}\right) \right)x_j^\left(i\right)</script>
其中，$j = 1, 2, 3, \cdots m \ $代表的是样本的第j维的特征，$x^{(i)}$代表的是n個樣本中的第i個樣本，$\eta$是学习率，是0~1之间的数，<a href="#func">$h_w(x^{(i)})$</a>为净输入，<a href="#func1">$\frac{\partial J(w)}{\partial w_j}$</a>为cost函数$J(w)$的梯度。
完整版的梯度下降：
<script type="math/tex; mode=display">\bf w_j: = w_j \ - \eta \sum_{i=1}^n (h_w(x^{(i)}) - y^{(i)})x_j^{(i)} \quad j = 1, 2, 3, \cdots m</script>
梯度上升法：</p>
<p>
<script type="math/tex; mode=display">\bf w_j: = w_j \ + \eta \sum_{i=1}^n (y^{(i)} - h_w(x^{(i)})x_j^{(i)}  \quad j = 1, 2, 3, \cdots m</script>
</p>
<p>因此，给定初始的模型系数（w）后，模型就会利用梯度下降算法自动学习到合适的模型系数w，拿到权重（模型）系数后，就可根据决策函数的输出对样本数据进行分类分析了。</p>
<p><a href="http://blog.csdn.net/dongtingzhizi/article/details/15962797">关于梯度下降过程向量化</a></p>
<p>vectorization后$\bf w$更新的步骤如下：
（1）求<script type="math/tex; mode=display">A = \bf X {\bullet} {\bf w}</script>；
（2）求E = h(A) - y；
（3）求<script type="math/tex; mode=display">w: = w -\eta {\bullet} X' {\bullet} E</script>，X'表示矩阵X的转置。</p>
<div id="func1" name="func1">
<img src="./image/func_parcial.png" alt="python machine learning">
</div>

<h2 id="pythonsklearn">python代码示例(调用sklearn)</h2>
<h4 id="sepal-length-and-petal-length-as-x">为了简化流程，从四个特征中抽取了两个特征组成特征矩阵，即选择了第一列花萼长度(sepal length) and 第三列花瓣长度(petal length) as X</h4>
<pre><code class="python">X = df.iloc[:, [0,2]].values  # .values　是将pandas的DataFrame或Series数据结构变成numpy的array的数组或矩阵类型
X.shape
X[-2:, :]
</code></pre>

<pre><code>array([[ 6.2,  5.4],
       [ 5.9,  5.1]])
</code></pre>
<pre><code class="python">fig, ax = plt.subplots(figsize=(7,7))
ax.scatter(X[:50, 0], X[:50, 1], marker=&quot;o&quot;, label=&quot;setosa&quot;, c=&quot;white&quot;, edgecolor=&quot;purple&quot;)
ax.scatter(X[50:100, 0], X[50:100, 1], c=&quot;blue&quot;, marker=&quot;x&quot;, label=&quot;versicolor&quot;)
ax.scatter(X[100:, 0], X[100:, 1], marker=&quot;^&quot;, label=&quot;virginica&quot;, c=&quot;white&quot;, edgecolor=&quot;red&quot;)
ax.set_xlabel(&quot;sepal length [cm]&quot;)
ax.set_ylabel(&quot;petal length [cm]&quot;)
ax.legend(loc=&quot;upper left&quot;)
ax.grid(True)
</code></pre>

<p><img alt="png" src="../output_24_0.png" /></p>
<pre><code class="python"># sns.scatterplot(x=&quot;petal length&quot;, y=&quot;sepal length&quot;, data=df, hue=&quot;iris&quot;,kind=&quot;point&quot;)
g = sns.FacetGrid(df, hue=&quot;iris&quot;, size=7, legend_out=False, hue_kws=dict(marker=[&quot;o&quot;, &quot;x&quot;, &quot;^&quot;]))
g.map(plt.scatter, &quot;sepal length&quot;, &quot;petal length&quot;,  alpha=.7)
g.add_legend();
</code></pre>

<p><img alt="png" src="../output_25_0.png" /></p>
<pre><code class="python">df.head(3)
</code></pre>

<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sepal length</th>
      <th>sepal width</th>
      <th>petal length</th>
      <th>petal width</th>
      <th>iris</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>Iris-setosa</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4.9</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>Iris-setosa</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.7</td>
      <td>3.2</td>
      <td>1.3</td>
      <td>0.2</td>
      <td>Iris-setosa</td>
    </tr>
  </tbody>
</table>
</div>

<pre><code class="python">y = df.iloc[:, 4]
y.unique()
# 将数据集的字符类型的数据转变成数值类型,Iris-setosa标记为０,Iris-Versicolor为1，
# Iris-virginica为2
y.replace(y.unique(), [0, 1, 2], inplace=True)
Y = y.values
# Y = y.values.reshape(-1, 1)
np.shape(Y)
# np.unique(Y)
# [u'Iris-setosa', u'Iris-versicolor', u'Iris-virginica'][0, 1, 2] 
</code></pre>

<pre><code>(150,)
</code></pre>
<pre><code class="python">X = df.iloc[:, [0,2]].values  # .values　是将pandas的DataFrame或Series数据结构变成numpy的array的数组或矩阵类型
# X[-2:, :]
np.shape(X)
</code></pre>

<pre><code>(150, 2)
</code></pre>
<h4 id="sklearnlogisticregressioniris">调用sklearn库的类LogisticRegression，实现对Iris训练集的学习，用交叉验证检验分类效果，最终将训练好的模型应用在测试集进行验证</h4>
<pre><code class="python">from sklearn.model_selection import train_test_split    
from sklearn.model_selection import cross_val_score 
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=0)
sc = StandardScaler()
sc.fit(X_train)
X_train_std = sc.transform(X_train)
X_test_std = sc.transform(X_test)
X_combined_std = np.vstack((X_train_std, X_test_std))
y_combined = np.hstack((y_train, y_test))
lr = LogisticRegression(C=100, penalty=&quot;l2&quot;, random_state=0, tol=1e-6)
lr.fit(X_train_std, y_train)
lr.predict_proba(X_test_std[0, :].reshape(1, -1))
# np.shape(X_test_std[0, :]),要整形为（１，　－１）
# np.shape(y_train)
y_pred = lr.predict(X_test_std)
print(&quot;Misclassified samples: %d&quot; % (y_test != y_pred).sum())
print(&quot;Accuracy: %.2f&quot; % accuracy_score(y_test,y_pred))
print(&quot;cross validation score: %s&quot; % cross_val_score(lr, X_train_std, y_train, cv=5))
print(&quot;mean cross validation score: %s&quot; % cross_val_score(lr, X_train_std, y_train, cv=5).mean())
</code></pre>

<pre><code>Misclassified samples: 1
Accuracy: 0.98
cross validation score: [ 0.95454545  1.          1.          0.9047619   0.94736842]
mean cross validation score: 0.961335156072
</code></pre>
<p>查看模型是否过拟合可以用learning curve来查看，过拟合现象表现为，在训练集上准确率得分比较高，但交叉验证集上得分较低，中间gap较大，一般是模型过于复杂导致，但一般随着样本量增加，过拟合会减弱。与之相反的还有欠拟合，即模型复杂度不够，训练集和交叉验证集的得分均较低。</p>
<p><img src="./image/overfitting and underfitting.png">
<b><span style="margin:0px, 100px, 0px, 120px;display:inline-block;">欠擬合</span><span style="margin:0px, 100px, 0px, 120px;display:inline-block;">最佳擬合</span><span style="margin:0px, 100px, 0px, 100px;display:inline-block;">過擬合</span></b>　</p>
<pre><code class="python">from sklearn.model_selection import learning_curve
# 用sklearn的learning_curve得到training_score和cv_score，使用matplotlib画出learning curve
def plot_learning_curve(estimator, title, X, y, ylim=None, cv=5, n_jobs=1, 
                        train_sizes=np.linspace(.05, 1., 20), verbose=0, plot=True):
    &quot;&quot;&quot;
    画出data在某模型上的learning curve.
    参数解释
    ----------
    estimator : 使用的分类器。
    title : 图的标题。
    X : 输入的feature，numpy类型
    y : 输入的target vector
    ylim : tuple格式的(ymin, ymax), 设定图像中纵坐标的最低点和最高点
    cv : 做cross-validation的时候，数据分成的份数，其中一份作为cv集，其余n-1份作为training(默认为3份)
    n_jobs : 并行的的任务数(默认1)
    &quot;&quot;&quot;
    train_sizes, train_scores, test_scores = learning_curve(
        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, verbose=verbose)

    train_scores_mean = np.mean(train_scores, axis=1)  # train_scores是一个２０行５列的ndarry,20为从样本取的不同比例的样本数据作为X, 而５表示５次交叉验证
    train_scores_std = np.std(train_scores, axis=1)
    test_scores_mean = np.mean(test_scores, axis=1)
    test_scores_std = np.std(test_scores, axis=1)

    if plot:
        plt.figure(figsize=(7,7))
        plt.title(title)
        if ylim is not None:
            plt.ylim(*ylim)
        plt.xlabel(&quot;samples&quot;)
        plt.ylabel(&quot;scores&quot;)
        # plt.gca().invert_yaxis() 例如y轴坐标3000-10000，调整为10000-3000来显示
        plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, 
                         alpha=0.2, color=&quot;b&quot;)
        plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, 
                         alpha=0.2, color=&quot;r&quot;)
        plt.plot(train_sizes, train_scores_mean, '^-', color=&quot;blue&quot;, label=&quot;train score&quot;)
        plt.plot(train_sizes, test_scores_mean, 'v-', color=&quot;red&quot;, label=&quot;cross_validation score&quot;)

        plt.legend(loc=&quot;best&quot;)

        # plt.draw()        
        # plt.gca().invert_yaxis()
        plt.grid(True)
        plt.show()
        # 取出最后一个
        # midpoint = ((train_scores_mean[-1] + train_scores_std[-1]) + (test_scores_mean[-1] - test_scores_std[-1])) / 2
        # diff = (train_scores_mean[-1] + train_scores_std[-1]) - (test_scores_mean[-1] - test_scores_std[-1])
        # return midpoint, diff

plot_learning_curve(lr, &quot;learning curve&quot;, X_train_std, y_train)
</code></pre>

<p><img alt="png" src="../output_32_0.png" /></p>
<h3 id="_3">决策边界示意图</h3>
<p><img src="./image/boundry1.png" style="display:inline-block;width:300px;height:300px;margin-top:20px;">
<img src="./image/boundry2.png" style="display:inline-block;width:300px;height:300px;"></p>
<pre><code class="python">from matplotlib.colors import ListedColormap
import matplotlib.pyplot as plt
def plot_decision_regions(X, Y, classifier, test_idx=None, resolution=0.02):
    # 对应分类标签
    y_maps = {&quot;1&quot;:&quot;Iris-versicolor&quot;, &quot;0&quot;:&quot;Iris-setosa&quot;, &quot;2&quot;: &quot;Iris-virginica&quot;}
    # setup marker generator and color map
    markers = (&quot;^&quot;, &quot;x&quot;, &quot;s&quot;, &quot;o&quot;, &quot;v&quot;)
    colors = (&quot;purple&quot;, &quot;red&quot;, &quot;blue&quot;, &quot;cyan&quot;, &quot;lightgreen&quot;, )  #&quot;gray&quot;
    cmap = ListedColormap(colors[:len(np.unique(Y))])
    # plot the decision surface    
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), np.arange(x2_min, x2_max, resolution))
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.figure(figsize=(8,8))
    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap) 

    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())
    # plot all samples
    for idx, cl in enumerate(np.unique(Y)):        
        plt.scatter(x=X[Y==cl, 0], y=X[Y==cl, 1], alpha=0.8, c=cmap(idx), marker=markers[idx],label=y_maps[str(cl)])

    # highlight test samples
    if test_idx:
        X_test, Y_test = X[test_idx, :], Y[test_idx]
        X_test, Y_test = X[test_idx, :], Y[test_idx]
        plt.scatter(X_test[:, 0], X_test[:, 1], c=&quot;&quot;, alpha=1.0, linewidth=1, marker=&quot;o&quot;, s=55, label=&quot;test set&quot;)

</code></pre>

<pre><code class="python">plot_decision_regions(X=X_combined_std, Y=y_combined, classifier=lr, test_idx=None)
plt.xlabel(&quot;sepal length [standardized] cm&quot;, fontsize=16)
plt.ylabel(&quot;petal length [standardized] cm&quot;, fontsize=14)
plt.legend(loc=&quot;upper left&quot;)
</code></pre>

<pre><code>&lt;matplotlib.legend.Legend at 0x7fdd1fee2f50&gt;
</code></pre>
<p><img alt="png" src="../output_36_1.png" /></p>
<h3 id="logistic-regression-params">Logistic Regression params</h3>
<p>class sklearn.linear_model.LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1.0,fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None,solver='liblinear', max_iter=100, multi_class='ovr', verbose=0,warm_start=False, n_jobs=1)</p>
<p>penalty: str "l1"或"l2"，正则化选择参数，即惩罚项的种类，是避免过拟合的方法之一，penalty参数的选择会影响我们损失函数优化算法的选择，即参数solver的选择，如果是L2正则化，那么4种可选的算法包括“newton-cg”, “lbfgs”, “liblinear”, “sag”都可以选择。但是如果penalty是L1正则化的话，就只能选择“liblinear”了。这是因为L1正则化的损失函数不是连续可导的，而“newton-cg”, “lbfgs”, “sag”这三种优化算法时都需要损失函数的一阶或者二阶连续导数。而“liblinear”并没有这个依赖</p>
<p>dual : bool, default: False
Dualor primal formulation. Dual formulation is only implemented for l2 penalty withliblinear solver. Prefer dual=False whenn_samples &gt; n_features.对偶或者原始方法。Dual只适用于正则化为l2 liblinear的情况，通常的数据集的样本数大于特征数的情况，使用默认为的False就OK了。</p>
<p>C : float, default: 1.0
Inverseof regularization strength; must be a positive float. Like in support vectormachines, smaller values specify stronger regularization.C为正则化系数λ的倒数，通常默认为1，C越小表示正则化的惩罚强度越大，目的就是为了防止过拟合</p>
<p>fit_intercept : bool, default: True
Specifiesif a constant (a.k.a. bias or intercept) should be added to the decisionfunction.是否存在截距，默认存在</p>
<p>intercept_scaling : float, default 1.仅在正则化项为"liblinear"，且fit_intercept设置为True时有用。</p>
<p><b>调参重点关注：</b></p>
<p>solver：</p>
<p>solver参数决定了我们对逻辑回归损失函数的优化方法，有四种算法可以选择，分别是：</p>
<p>a) liblinear：使用了开源的liblinear库实现，内部使用了坐标轴下降法来迭代优化损失函数。默认选择的算法</p>
<p>b) lbfgs：拟牛顿法的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。</p>
<p>c) newton-cg：也是牛顿法家族的一种，利用损失函数二阶导数矩阵即海森矩阵来迭代优化损失函数。</p>
<p>d) sag：即随机平均梯度下降，是梯度下降法的变种，和普通梯度下降法的区别是每次迭代仅仅用一部分的样本来计算梯度，适合于样本数据多的时候。</p>
<p>分类方式选择参数：</p>
<p>multi_class : str, "ovr", "multinomial", default: "ovr"，ovr即one-vs-rest(OvR)，而multinomial即many-vs-many(MvM)。如果是二元逻辑回归，ovr和multinomial并没有任何区别，区别主要在多元逻辑回归上。</p>
<p>类型权重参数：（考虑误分类代价敏感、分类类型不平衡的问题）</p>
<p>class_weight : dictor ‘balanced’, default: None </p>
<p>class_weight参数用于标示分类模型中各种类型的权重，可以不输入，即表示所有类型的权重一样。如果选择输入的话，可以选择balanced让类库自己计算类型权重，或者我们自己输入各个类型的权重，比如对于0,1的二元模型，我们可以定义class_weight={0:0.9, 1:0.1}，这样类型0的权重为90%，而类型1的权重为10%。 </p>
<p>样本权重参数：
sample_weight（fit函数参数）</p>
<p>当样本是高度失衡的，导致样本不是总体样本的无偏估计，从而可能导致我们的模型预测能力下降。遇到这种情况，我们可以通过调节样本权重来尝试解决这个问题。调节样本权重的方法有两种，第一种是在class_weight使用balanced。第二种是在调用fit函数时，通过sample_weight来自己调节每个样本权重。在scikit-learn做逻辑回归时，如果上面两种方法都用到了，那么样本的真正权重是class_weight*sample_weight.</p>
<p>max_iter : int, default: 100</p>
<p>Usefulonly for the newton-cg, sag and lbfgs solvers. Maximum number of iterationstaken for the solvers to converge.仅在正则化优化算法为newton-cg, sag and lbfgs 才有用，算法收敛的最大迭代次数。</p>
<p>verbose : int, default: 0</p>
<p>Forthe liblinear and lbfgs solvers set verbose to any positive number forverbosity.日志冗长度int：冗长度；0：不输出训练过程；1：偶尔输出； &gt;1：对每个子模型都输出</p>
<p>random_state : </p>
<p>int seed, RandomState instance, default: None。The seed of the pseudo random number generator touse when shuffling the data. Used only in solvers ‘sag’ and ‘liblinear’.随机数种子，默认为无，仅在正则化优化算法为sag,liblinear时有用。</p>
<p>warm_start : bool, default: False</p>
<p>是否热启动，如果是，则下一次训练是以追加树的形式进行（重新使用上一次的调用作为初始化），bool：热启动，False：默认值</p>
<p>n_jobs : int, default: 1</p>
<p>Numberof CPU cores used during the cross-validation loop. If given a value of -1, allcores are used.并行数，int：个数；-1：跟CPU核数一致；1:默认值</p>
<p>tol : float, default: 1e-4 Tolerance for stopping criteria.迭代终止判据的误差范围。</p>
<p><a href="http://blog.csdn.net/cherdw/article/details/54891073">参考</a></p>
<h3 id="logistic">Logistic 预测</h3>
<pre><code class="python"># 用Logistic回归建模  
# clf = linear_model.LogisticRegression(C=1.0, penalty=&quot;l1&quot;, solver='liblinear', tol=1e-6)  # tol : float, default: 1e-4 Tolerance for stopping criteria.迭代终止判据的误差范围。
lr_model= LogisticRegression(C = 1.0,  
                              penalty = 'l2')  
lr_model.fit(train_X,train_y)  
# 给出交叉验证集的预测结果，并输出评估的准确率、召回率、F1值  
pred_test= lr_model.predict(test_X)  
printclassification_report(test_y, pred_test)  
# 输出测试集用户逾期还款概率，predict_proba会输出两个概率，即分别分类为‘0’和‘1’的概率，这里只取‘1’的概率  
pred= lr_model.predict_proba(test)  
result= pd.DataFrame(pred)  
result.index= test.index  
result.columns= ['0', 'probability']  
result.drop('0',  
            axis = 1,  
            inplace = True)  
printresult.head(5)  
#输出结果  
result.to_csv('result.csv')  
</code></pre>

<p><b>python machine learning</b>－python机器学习入门书<a href="http://download.csdn.net/detail/qq_30096641/9853462">中文版</a></p>
<p><b>Python机器学习——预测分析核心算法(machine learning in python--essential techniques for predictive analysis)<b> </p>
<p><b>统计学习方法</b>——李航</p>
<p><b>機器學習簡單介紹</b>
<a href="http://blog.jobbole.com/92021/">10 种机器学习算法的要点（附 Python 和 R 代码）</a></p>
<h2 id="_4">参考文献</h2>
<p>【机器学习笔记1】Logistic回归总结 http://blog.csdn.net/dongtingzhizi/article/details/15962797</p>
<p>【机器学习笔记2】Linear Regression总结 http://blog.csdn.net/dongtingzhizi/article/details/16884215</p>
<p>Logistic Regression 模型简介　https://tech.meituan.com/intro_to_logistic_regression.html</p>
<p>逻辑回归算法的原理及实现(LR) http://www.cnblogs.com/nxld/p/6124235.html</p>
<p>逻辑回归（Logistic regression）详解-并用scikit-learn训练逻辑回归拟合Iris数据集  http://blog.csdn.net/xlinsist/article/details/51289825</p>
<p>Sklearn-LogisticRegression逻辑回归 http://blog.csdn.net/cherdw/article/details/54891073</p>
<p>机器学习算法与Python实践之（七）逻辑回归（Logistic Regression） http://blog.csdn.net/zouxy09/article/details/20319673</p>
<p>机器学习算法与Python实践之（七）逻辑回归（Logistic Regression http://blog.csdn.net/wenyusuran/article/details/25824011</p>
<p>正则化方法：L1和L2 regularization、数据集扩增、dropout http://blog.csdn.net/u012162613/article/details/44261657</p>
<p>LaTeX 各种命令，<script type="math/tex; mode=display">x_j^\left(i\right)</script> 符号 http://blog.csdn.net/anxiaoxi45/article/details/39449445</p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
      
        <a href="../../data_analysis/lagou_job_analysis_forshow/" class="btn btn-neutral" title="拉勾招聘职位数据分析"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
	  
        </div>
      </div>

    </section>
    
  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../../data_analysis/lagou_job_analysis_forshow/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
    </span>
</div>
    <script src="../../js/theme.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

</body>
</html>
